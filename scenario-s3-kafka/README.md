# Scenario S3: Kafka + Schema Registry æ•´åˆæ¸¬è©¦

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å ´æ™¯å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š
- ä½¿ç”¨ Testcontainers ç®¡ç† Kafka å’Œ Schema Registry å®¹å™¨
- ä½¿ç”¨ Avro é€²è¡Œè¨Šæ¯åºåˆ—åŒ–
- å¯¦ä½œ Schema ç‰ˆæœ¬æ¼”é€²ï¼ˆBackward Compatibilityï¼‰
- æ¸¬è©¦ Kafka è¨Šæ¯çš„åˆ†å€é †åºä¿è­‰
- ä½¿ç”¨ KRaft æ¨¡å¼ï¼ˆç„¡éœ€ ZooKeeperï¼‰

## ç’°å¢ƒéœ€æ±‚

- Java 21+
- Docker Desktop
- Gradle 8.x

## æ¦‚è¿°

S3 å ´æ™¯å±•ç¤º Apache Kafka å’Œ Confluent Schema Registry çš„æ•´åˆæ¸¬è©¦ï¼ŒåŒ…å«ï¼š
- **Kafka Producer/Consumer** - è¨Šæ¯ç™¼é€èˆ‡æ¥æ”¶
- **Avro Serialization** - å¼·å‹åˆ¥è¨Šæ¯æ ¼å¼
- **Schema Evolution** - å‘å¾Œç›¸å®¹çš„ Schema æ¼”é€²
- **Partition Ordering** - ç›¸åŒ Key çš„è¨Šæ¯é †åºä¿è­‰

## æŠ€è¡“å…ƒä»¶

| å…ƒä»¶ | å®¹å™¨æ˜ åƒ | ç”¨é€” |
|------|----------|------|
| Kafka | confluentinc/cp-kafka:7.6.0 | è¨Šæ¯ä½‡åˆ—ï¼ˆKRaft æ¨¡å¼ï¼‰ |
| Schema Registry | confluentinc/cp-schema-registry:7.6.0 | Avro Schema ç®¡ç† |

## æ ¸å¿ƒæ¦‚å¿µ

### 1. KRaft æ¨¡å¼

Kafka 3.0+ æ”¯æ´ KRaft æ¨¡å¼ï¼Œç„¡éœ€ ZooKeeperï¼š

```java
KafkaContainer kafka = new KafkaContainer(
    DockerImageName.parse("confluentinc/cp-kafka:7.6.0"))
    .withKraft()  // å•Ÿç”¨ KRaft æ¨¡å¼
    .withReuse(true);
```

### 2. Avro Schema

ä½¿ç”¨ Avro å®šç¾©å¼·å‹åˆ¥è¨Šæ¯æ ¼å¼ï¼š

```json
{
  "type": "record",
  "name": "OrderEvent",
  "namespace": "com.example.s3.avro",
  "fields": [
    {"name": "orderId", "type": "string"},
    {"name": "customerId", "type": "string"},
    {"name": "amount", "type": "double"},
    {"name": "status", "type": {"type": "enum", "name": "OrderStatus",
        "symbols": ["CREATED", "CONFIRMED", "SHIPPED", "DELIVERED", "CANCELLED"]}},
    {"name": "timestamp", "type": "long"}
  ]
}
```

### 3. Schema Evolutionï¼ˆå‘å¾Œç›¸å®¹ï¼‰

V2 Schema æ–°å¢ nullable æ¬„ä½ï¼Œç¢ºä¿å‘å¾Œç›¸å®¹ï¼š

```json
{
  "name": "productName",
  "type": ["null", "string"],
  "default": null,
  "doc": "Product name - optional field added in v2"
}
```

**å‘å¾Œç›¸å®¹è¦å‰‡**ï¼š
- âœ… æ–°å¢ nullable æ¬„ä½ï¼ˆæœ‰é è¨­å€¼ï¼‰
- âœ… åˆªé™¤æ¬„ä½ï¼ˆæ–°æ¶ˆè²»è€…å¿½ç•¥èˆŠæ¬„ä½ï¼‰
- âŒ æ–°å¢ required æ¬„ä½ï¼ˆç„¡é è¨­å€¼ï¼‰
- âŒ è®Šæ›´æ¬„ä½å‹åˆ¥

### 4. Partition Ordering

ç›¸åŒ Key çš„è¨Šæ¯ä¿è­‰é †åºï¼š

```java
// ä½¿ç”¨ orderId ä½œç‚º Keyï¼Œç¢ºä¿åŒä¸€è¨‚å–®çš„äº‹ä»¶æŒ‰é †åºè™•ç†
producer.send(new ProducerRecord<>("order-events", orderId, event));
```

## æ•™å­¸æ­¥é©Ÿ

### æ­¥é©Ÿ 1ï¼šç†è§£å°ˆæ¡ˆçµæ§‹

```
scenario-s3-kafka/
â”œâ”€â”€ src/main/java/com/example/s3/
â”‚   â”œâ”€â”€ S3Application.java
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ KafkaConfig.java         # Kafka é…ç½®èˆ‡ Topic å»ºç«‹
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ OrderEventProducer.java  # Kafka Producer
â”‚   â””â”€â”€ consumer/
â”‚       â””â”€â”€ OrderEventConsumer.java  # Kafka Consumer
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.yml
â”‚   â””â”€â”€ avro/
â”‚       â”œâ”€â”€ order-event-v1.avsc      # V1 Schema
â”‚       â””â”€â”€ order-event-v2.avsc      # V2 Schemaï¼ˆå‘å¾Œç›¸å®¹ï¼‰
â””â”€â”€ src/test/java/com/example/s3/
    â”œâ”€â”€ S3TestApplication.java        # æ¸¬è©¦é…ç½®
    â”œâ”€â”€ BaseKafkaIT.java              # åŸºåº•æ¸¬è©¦é¡åˆ¥
    â”œâ”€â”€ KafkaProducerConsumerIT.java  # Producer/Consumer æ¸¬è©¦
    â””â”€â”€ SchemaEvolutionIT.java        # Schema æ¼”é€²æ¸¬è©¦
```

### æ­¥é©Ÿ 2ï¼šåŸ·è¡Œæ¸¬è©¦

```bash
# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
./gradlew :scenario-s3-kafka:test

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦é¡åˆ¥
./gradlew :scenario-s3-kafka:test --tests "KafkaProducerConsumerIT"
./gradlew :scenario-s3-kafka:test --tests "SchemaEvolutionIT"
```

### æ­¥é©Ÿ 3ï¼šè§€å¯Ÿ Schema æ¼”é€²

1. è¨»å†Š V1 Schema
2. ç™¼é€ V1 è¨Šæ¯
3. è¨»å†Š V2 Schemaï¼ˆå‘å¾Œç›¸å®¹ï¼‰
4. ç™¼é€ V2 è¨Šæ¯ï¼ˆåŒ…å«æ–°æ¬„ä½ï¼‰
5. V1 æ¶ˆè²»è€…ä»å¯è®€å– V2 è¨Šæ¯

## ç³»çµ±æ¶æ§‹

```mermaid
flowchart TB
    subgraph Test["ğŸ§ª æ¸¬è©¦å®¹å™¨ç’°å¢ƒ"]
        subgraph App["Spring Boot Application"]
            Producer["OrderEventProducer\n(Avro Serializer)"]
            Consumer["OrderEventConsumer\n(Avro Deserializer)"]
        end

        subgraph Containers["Testcontainers"]
            Kafka["Kafka\n(KRaft Mode)\n3 Partitions"]
            SR["Schema Registry\n(Avro Schemas)"]
        end
    end

    Producer -->|"publish\n(orderId as key)"| Kafka
    Kafka --> Consumer
    Producer -.->|"register schema"| SR
    Consumer -.->|"fetch schema"| SR

    style Test fill:#f0f8ff,stroke:#4169e1
    style App fill:#e6ffe6,stroke:#228b22
    style Containers fill:#fff0f5,stroke:#dc143c
```

## Schema æ¼”é€²æµç¨‹

```mermaid
flowchart LR
    subgraph V1["Schema V1"]
        V1F["orderId\ncustomerId\namount\nstatus\ntimestamp"]
    end

    subgraph V2["Schema V2 (Backward Compatible)"]
        V2F["orderId\ncustomerId\namount\nstatus\ntimestamp\n+ productName (nullable)\n+ quantity (nullable)"]
    end

    V1 -->|"evolution"| V2

    style V1 fill:#ffe4e1,stroke:#cd5c5c
    style V2 fill:#e0ffe0,stroke:#32cd32
```

## æ¸¬è©¦é¡åˆ¥èªªæ˜

### KafkaProducerConsumerIT

| æ¸¬è©¦æ¡ˆä¾‹ | èªªæ˜ |
|----------|------|
| `shouldProduceAndConsumeOrderEvent` | åŸºæœ¬è¨Šæ¯ç™¼é€èˆ‡æ¥æ”¶ |
| `shouldMaintainEventOrderingForSamePartitionKey` | ç›¸åŒ Key çš„é †åºä¿è­‰ |
| `shouldHandleMultipleOrdersConcurrently` | ä½µç™¼è¨Šæ¯è™•ç† |
| `shouldDistributeEventsAcrossPartitions` | è¨Šæ¯åˆ†æ•£åˆ°ä¸åŒåˆ†å€ |

### SchemaEvolutionIT

| æ¸¬è©¦æ¡ˆä¾‹ | èªªæ˜ |
|----------|------|
| `shouldRegisterV1Schema` | è¨»å†Š V1 Schema |
| `shouldProduceAndConsumeV1Events` | V1 è¨Šæ¯æµç¨‹ |
| `shouldEvolveSchemaWithBackwardCompatibility` | V1 â†’ V2 æ¼”é€² |
| `shouldProduceV2EventsWithNewFields` | V2 è¨Šæ¯å«æ–°æ¬„ä½ |
| `shouldProduceV2EventsWithNullOptionalFields` | V2 è¨Šæ¯ nullable æ¬„ä½ |
| `shouldHandleMixedV1AndV2Events` | æ··åˆç‰ˆæœ¬è™•ç† |
| `shouldRejectNonBackwardCompatibleSchema` | æ‹’çµ•ä¸ç›¸å®¹ Schema |
| `shouldListAllSchemaVersions` | åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬ |

## ç¨‹å¼ç¢¼ç¯„ä¾‹

### Producer

```java
@Service
public class OrderEventProducer {

    private final KafkaTemplate<String, GenericRecord> kafkaTemplate;

    public void sendOrderEvent(String orderId, GenericRecord event) {
        // ä½¿ç”¨ orderId ä½œç‚º Keyï¼Œç¢ºä¿é †åº
        kafkaTemplate.send("order-events", orderId, event)
            .whenComplete((result, ex) -> {
                if (ex != null) {
                    log.error("Failed to send event", ex);
                } else {
                    log.info("Event sent to partition {}",
                        result.getRecordMetadata().partition());
                }
            });
    }
}
```

### Consumer

```java
@Service
public class OrderEventConsumer {

    @KafkaListener(topics = "${app.kafka.topics.order-events}")
    public void handleOrderEvent(GenericRecord record) {
        String orderId = record.get("orderId").toString();
        String status = record.get("status").toString();

        // V2 optional æ¬„ä½å¯èƒ½ç‚º null
        Object productName = record.get("productName");

        log.info("Received order event: {} - {}", orderId, status);
    }
}
```

### Schema ç›¸å®¹æ€§æ¸¬è©¦

```java
@Test
void shouldRejectNonBackwardCompatibleSchema() throws Exception {
    // Given - å·²è¨»å†Š V1 Schema
    String subject = "order-events-test-value";
    schemaRegistryClient.register(subject, new AvroSchema(v1Schema));

    // When - å˜—è©¦è¨»å†Šä¸ç›¸å®¹çš„ Schemaï¼ˆæ–°å¢ required æ¬„ä½ï¼‰
    String incompatibleSchema = """
        {
          "type": "record",
          "name": "OrderEvent",
          "fields": [
            {"name": "orderId", "type": "string"},
            {"name": "requiredNewField", "type": "string"}  // æ²’æœ‰é è¨­å€¼ï¼
          ]
        }
        """;
    Schema badSchema = new Schema.Parser().parse(incompatibleSchema);

    // Then - Schema Registry æ‡‰æ‹’çµ•
    boolean isCompatible = schemaRegistryClient.testCompatibility(
        subject, new AvroSchema(badSchema));
    assertThat(isCompatible).isFalse();
}
```

## å¸¸è¦‹å•é¡Œ

### Q1: Schema ç›¸å®¹æ€§æª¢æŸ¥å¤±æ•—
**å•é¡Œ**: æ–° Schema è¢« Schema Registry æ‹’çµ•
**è§£æ±º**: ç¢ºä¿æ–°å¢æ¬„ä½æœ‰é è¨­å€¼ï¼ˆ`"default": null`ï¼‰ï¼Œä¸”å‹åˆ¥ç‚º unionï¼ˆ`["null", "string"]`ï¼‰

### Q2: è¨Šæ¯é †åºéŒ¯äº‚
**å•é¡Œ**: åŒä¸€è¨‚å–®çš„äº‹ä»¶è™•ç†é †åºä¸æ­£ç¢º
**è§£æ±º**: ç¢ºä¿ä½¿ç”¨ç›¸åŒçš„ partition keyï¼ˆå¦‚ orderIdï¼‰

### Q3: æ¸¬è©¦é–“ Schema è¡çª
**å•é¡Œ**: ä¸åŒæ¸¬è©¦è¨»å†Šçš„ Schema äº’ç›¸å¹²æ“¾
**è§£æ±º**: åœ¨ `@BeforeEach` ä¸­æ¸…é™¤ Schema Registry subjects

### Q4: å®¹å™¨å•Ÿå‹•é †åºå•é¡Œ
**å•é¡Œ**: Schema Registry åœ¨ Kafka ä¹‹å‰å•Ÿå‹•å°è‡´é€£ç·šå¤±æ•—
**è§£æ±º**: ä½¿ç”¨ `dependsOn(kafkaContainer)` ç¢ºä¿å•Ÿå‹•é †åº

## é©—æ”¶æ¨™æº–

- âœ… è¨Šæ¯æˆåŠŸç™¼é€èˆ‡æ¥æ”¶
- âœ… åˆ†å€é †åºä¿è­‰ï¼ˆç›¸åŒ Keyï¼‰
- âœ… Schema V1 â†’ V2 å‘å¾Œç›¸å®¹æ¼”é€²
- âœ… æ··åˆç‰ˆæœ¬è¨Šæ¯è™•ç†
- âœ… ä¸ç›¸å®¹ Schema è¢«æ‹’çµ•

## è¨­å®šåƒè€ƒ

### application.yml

```yaml
spring:
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      auto-offset-reset: earliest
    properties:
      schema.registry.url: ${SCHEMA_REGISTRY_URL:http://localhost:8081}
      specific.avro.reader: false  # ä½¿ç”¨ GenericRecord
```

## å»¶ä¼¸å­¸ç¿’

- [S4-CDC](../scenario-s4-cdc/): CDC è®Šæ›´è³‡æ–™æ“·å–
- [Confluent Schema Registry æ–‡ä»¶](https://docs.confluent.io/platform/current/schema-registry/)
- [Avro è¦æ ¼](https://avro.apache.org/docs/current/spec.html)
- [Kafka Partition è¨­è¨ˆ](https://kafka.apache.org/documentation/#design_partitioningstrategy)
